---
layout: wiki
title: 本科实习/RA：基于达闼的云端智能平台的数字孪生开发
cate1: 数字孪生
cate2: 
description: 时间久远，部分资料遗失，见谅
keywords: 仿真，数字孪生
---











# 写在前面

达闼的实习是在**我本科期间作为科研助理的时候**，因为达闼科技是我们华科电信学院的院长黄晓庆创立的专注云端智能机器人的公司，所以找导师进实验室时，就有了这次实习经历。同时，这也是我第一次接触工业级的数字孪生项目。**实习期间，我干的事概括来说就是，学会使用达闼的云端智能机器人开发平台 CloudMinds HARIX （基于虚幻4开发），分解任务流程，实现机器人从输入取水指令到完成取水动作的过程，同时，基于达闼的数字孪生，也完成了从虚拟仿真到真实任务的映射**。



 因为时间有点长了，我基于手头的资料和工程重新整理出了大致的流程。



# 任务分析与分解



从输入取水指令到完成取水动作包含远程云端与本地的多次交互。 首先，达闼的云端拿到我们发送的指令，肯定要经过语言处理与匹配，得到取水命令，使得机器人开始执行取水的任务。 接下来则是，依次分解取水的各个阶段并执行。



## 关于语言处理



我们使用AIML. AIML，全名为Artificial Intelligence Markup Language是一种创建自然语言软件代理的XML语言，是由Richard Wallace和世界各地的自由软件社区在1995年至2002年发明的。
## 关于解析器
​	aiml是一种语言，需要对应的解释器去对该类语言做解析，我现在使用的是aiml 2.0的解析器。

## aiml的基础结构

![image-20230317214030427](C:\Users\xue\AppData\Roaming\Typora\typora-user-images\image-20230317214030427.png)



有了aiml文件和aiml解析器就可以构建一个简单的机器人

### aiml匹配规则
#### 1.通配符：
上文pattern的形式决定了最后query的匹配结果，如果直接全部为自然语言，那么只有完全一样的query才能得到响应，这和预期不符，为此有‘*’和‘#’等通配符，帮助我们提升匹配的泛化度，其中：
‘*’：该字符表示’1+‘个字符，即*位置可替换为1个或者多个字符， 

#### 2.star标签

![image-20230317214200335](C:\Users\xue\AppData\Roaming\Typora\typora-user-images\image-20230317214200335.png)



user：能介绍一下bill吗
bot：bill是达闼的老板

#### 3.<random> <li>

![image-20230317214246181](C:\Users\xue\AppData\Roaming\Typora\typora-user-images\image-20230317214246181.png)

这组标签主要用于泛化机器人的回答，是的答案更具多样性，更生动


4. #### <think>标签

	存储变量，且不返回给用户，放置在template元素里面，表示一旦用户的输入匹配到该category时，再回复应答的同时，给一个变量赋值，以后就可以用<get name=”topic”/>来取出先前记住的内容。
5. #### <set><get>

	这一组标签是用来设置变量和获取变量值的，使得机器人具有一定的记忆能力

![image-20230317214309613](C:\Users\xue\AppData\Roaming\Typora\typora-user-images\image-20230317214309613.png)








7. #### <srai>

	srai标签的使用可帮助我们精简数据模板，主要是通过在一个template中引用某个pattern对应的template
	表示<srai>里面的话会被当作是用户输入，从新查找匹配模式，直到找到非<srai>定义的回复。例如： 
<srai>我 是 <star/></srai>，那么机器人会把“我 是 *”当作是用户输入来从新查找匹配模式。（PS：srai不能很好的匹配中文）



**对于取水指令，我们写好对应的AIML模板，上传云端，设置好云端对应的响应事件，就可以针对于相对随意的命令 如， “可以帮忙取水”，“来接个水”之类的做出正确的回应**。



##  取水任务的分解



**下面是Leader带我时做的流程图（Leader太强啦，Leader的强我以前看不出来，现在感觉至少得是个UE4全栈开发，膜拜~）**



*以下这个是结合硬件实体机器人的流程，环节包括CCU(云端中心)，RCU(机器人)，UE4客户端，UE4BP，UE服务器，UE4代理，语音识别代理，以及封装的蓝图事件（接待，导航，抓取，视觉）*





![抓取物品流程图](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/%E6%8A%93%E5%8F%96%E7%89%A9%E5%93%81%E6%B5%81%E7%A8%8B%E5%9B%BE.png)



而**实际我们需要做的只是，正确的在HARIX平台（也就是Unreal编辑器）中，梳理好封装的蓝图事件（接待，导航，抓取，视觉）的逻辑即可，类似于蓝图连连看**。



### 自定义蓝图事件接口一览表





*其他Unreal自带的就不放了，下面放一下HARIX平台自定义的蓝图接口一览*：



![image-20230316020432564](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/image-20230316020432564.png)

![image-20230316020520811](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/image-20230316020520811.png)





### 最终代码

**只能展示在ThirdPlayer蓝图的部分连连看逻辑了，其他的没保存**Orz

PS：当时做的时候，根本不明白他这个值得学习的蓝图和代码基本自定义事件里封装着的。。。









![2](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/2.png)





![3](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/3.png)







# 视频实录：



<video src="https://github.com/U201613306/U201613306.github.io/raw/master/images/robot.mp4"></video>



[视频下载](https://github.com/U201613306/U201613306.github.io/raw/master/images/robot.mp4)









# 真机同步





![image-20230316015106742](https://github.com/U201613306/U201613306.github.io/raw/master/images/data/image-20230316015106742.png)

放一张真机的图，别问为什么没有真机同步视频（当时拍了，时间太久了，这个视频丢了Orz）







# 心得





  ***达闼的这个数字孪生项目中，我遇到了人非常好的leader，同时，基于他们开发的（当时还在测试）HIRAX云端机器人平台，让本来对此了解不多的我，在梳理清楚实验的步骤和通信过程后，也能借助连连看系统，完成输入相关指令到执行的全过程，那种成就感我到现在还铭记于心。 这也是我对虚幻引擎，对虚拟现实，对图形学相关开始感兴趣的开始。***

  ***之后，研究生期间，我与数字孪生的缘分不断233，和组里真正的参与数字孪生的大型项目开发（详见 固定翼着舰平台）。 我也慢慢明白，对于这一类数字孪生的仿真平台，最重要的，也是最难做到的，其实是虚拟和现实的一致性。 比如，固定翼着舰平台的项目要求的至少软件在环的仿真标准。因为用于测试算法的仿真，底层实现必须是基于现实硬件的模拟，而不是简单的指定真值。没有这类考虑的仿真平台，注定只是作秀，不具有实际价值。  达闼的工作的挑战之处就是他开发的这个HARIX云端平台和实际机器人位姿，动作的同步。根据我实习的经历来看，在他设定的状态下，做的是相当好的。***